{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-openai rank_bm25 tf-keras-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Read text files and the chunk. Workflow: read -> chunk a doc -> embed -> chroma. Do not chunk all the docs at once. https://docs.trychroma.com/guides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "import os\n",
    "# def fetch_info(path: str):\n",
    "# \tloader = PyPDFLoader(\n",
    "# \t\tpath,\n",
    "# \t)\n",
    "\n",
    "# \ttext = loader.load()\n",
    "# \treturn text\n",
    "\n",
    "def fetch_info(path: str):\n",
    "\tloader = TextLoader(\n",
    "\t\tpath,\n",
    "\t)\n",
    "\ttext = loader.load()\n",
    "\treturn text\n",
    "# directory = \"Data\"\n",
    "# for filename in os.listdir(directory):\n",
    "# \tloader = TextLoader(os.path.join(directory, filename))\n",
    "# \ttext = loader.load()\n",
    "# \tprint(text[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=50)\n",
    "\n",
    "directory = \"/Users/dexter/Vortex9/AI_Assistant_for_docs/Data\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "all_info = []\n",
    "for file in files:\n",
    "\tdoc = fetch_info(directory + \"/\" + file)[0].page_content\n",
    "\tall_info.append(doc)\n",
    "\n",
    "\n",
    "# chunks = []\n",
    "# for text in all_info:\n",
    "# \tchunks.extend(text_splitter.split_text(text))\n",
    "\n",
    "\n",
    "# # chunks = text_splitter.split_text(text)\n",
    "\n",
    "# text_pattern = re.compile(r'[a-zA-Z0-9@.,:;!?$€£¥+\\*/=#&%(){}\\[\\]<>\\'\"]+')\n",
    "\n",
    "# filtered_chunks = []\n",
    "# for chunk in chunks:\n",
    "# \textracted_text = ' '.join(text_pattern.findall(chunk))\n",
    "# \tif extracted_text:\n",
    "# \t\tfiltered_chunks.append(extracted_text)\n",
    "\n",
    "# # for i, chunk in enumerate(filtered_chunks):\n",
    "# # \tprint(f\"Chunk {i+1}: {chunk}\")\n",
    "\n",
    "# print(len(filtered_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "        <document>\n",
    "        {doc_content}\n",
    "        </document>\n",
    "        \"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "        Here is the chunk we want to situate within the whole document\n",
    "        <chunk>\n",
    "        {chunk_content}\n",
    "        </chunk>\n",
    "\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "        Answer only with the succinct context and nothing else.\n",
    "        \"\"\"\n",
    "\n",
    "def generate_chunk_context(doc:str, chunk:str):\n",
    "  messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc) + CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "           \t\t\t\"cache_control\" : {\"type\": \"ephemeral\"} # Prompt caching\n",
    "                },\n",
    "  ]\n",
    "  response = llm.invoke(messages)\n",
    "  return response.content\n",
    "\n",
    "final_chunks = []\n",
    "whole_doc = \" \".join(all_info)\n",
    "for chunk in all_info:\n",
    "  chunk_context = generate_chunk_context(whole_doc, chunk)\n",
    "  final_chunk = chunk_context + \" \" + chunk\n",
    "  final_chunks.append(final_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenized_chunks = [re.findall(r'\\w+', chunk.lower()) for chunk in final_chunks]\n",
    "\n",
    "#Creating BM25 index\n",
    "bm25 = BM25Okapi(tokenized_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "\n",
    "client = chromadb.PersistentClient(\"chroma.db\")\n",
    "\n",
    "collection_name = \"doc_collection\"\n",
    "\n",
    "collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "\tfor i, chunk in enumerate(chunks):\n",
    "\t\tembeddings = model.embed_documents(chunk)\n",
    "\t\tcollection.add(\n",
    "\t\t\tdocuments=[chunk],\n",
    "\t\t\tembeddings=[embeddings],\n",
    "\t\t\tmetadatas=[{\"product_id\": i}], \n",
    "\t\t\tids=[str(i)]\n",
    "\t\t)\n",
    "\n",
    "generate_embeddings(final_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hello\"\n",
    "\n",
    "tokenized_query = re.findall(r'\\w+', query.lower())\n",
    "\n",
    "#bm25 scores\n",
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "bm25_scores = sorted(bm25_scores, reverse=True)[:10]\n",
    "print(\"Top 5 BM25 scores are:\")\n",
    "i=0\n",
    "for score in bm25_scores:\n",
    "\tprint(f\"{i+1}. {score}\")\n",
    "\ti+=1\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Top chunks are:\")\n",
    "for i, chunk in enumerate(bm25.get_top_n(tokenized_query, final_chunks, n=5)):\n",
    "\tprint(f\"{i+1}. {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding for the query\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Query ChromaDB with the query embedding to find the most relevant chunks\n",
    "chroma_results = collection.query(\n",
    "    query_embeddings=[query_embedding.tolist()],\n",
    "    n_results=10  # Retrieve top 10 results based on embedding similarity\n",
    ")\n",
    "\n",
    "chroma_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_scores = []\n",
    "chroma_ids = []\n",
    "\n",
    "chroma_scores = chroma_results['distances'][0]\n",
    "chroma_ids = chroma_results['ids'][0]\n",
    "\n",
    "chroma_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize BM25 scores\n",
    "bm25_scores_normalized = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))\n",
    "\n",
    "# Normalize ChromaDB distances (lower distance means higher similarity, so invert it)\n",
    "chroma_scores_inverted = [1 - score for score in chroma_scores]\n",
    "chroma_scores_normalized = (chroma_scores_inverted - np.min(chroma_scores_inverted)) / (np.max(chroma_scores_inverted) - np.min(chroma_scores_inverted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve scores from both sources for each chunk\n",
    "combined_scores = np.zeros(len(final_chunks))\n",
    "\n",
    "all_scores = zip(chroma_ids, chroma_scores_normalized)\n",
    "\n",
    "#open zip \n",
    "all_scores = list(all_scores)\n",
    "print(all_scores)\n",
    "\n",
    "for i, (chunk_id, chroma_score) in enumerate(all_scores):\n",
    "\tcombined_scores[int(chunk_id)] = 0.5 * bm25_scores_normalized[i] + 0.5 * chroma_score  # Adjust the weight as needed\n",
    "\n",
    "\n",
    "# # Combine BM25 and Chroma scores with a weighting factor\n",
    "# for i, chunk_id in enumerate(chroma_ids):\n",
    "# \tcombined_scores[int(chunk_id)] = 0.5 * bm25_scores_normalized[int(chunk_id)] + 0.5 * chroma_scores_normalized[i]  # Adjust the weight as needed\n",
    "\n",
    "# # Sort the combined scores to get the top N results\n",
    "top_n_indices = np.argsort(combined_scores)[::-1][:10]\n",
    "\n",
    "# # Fetch the top N chunks\n",
    "top_chunks = [final_chunks[i] for i in top_n_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response(query, top_chunks):\n",
    "    # Combine the top chunks into a single prompt\n",
    "    combined_text = \"\\n\\n\".join(top_chunks)\n",
    "\n",
    "    # Generate LLM response based on the retrieved chunks\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "                    Context:\\n{combined_text}\\n\\nQuery: {query}\\n\\nInstructions:  \n",
    "                    If the query is directly related to the context provide, provide an accurate and relevant answer based on the context. \n",
    "                    If the query is a general or unrelated question (e.g., greetings, or questions unrelated to the document),make up a response based on your data and respond in a generalized manner without referring to the document context.\n",
    "                    For answers to question you do not know refer to your data, or search the web for the answer.\n",
    "                    Examples:\n",
    "\n",
    "                    1. Query: \"What is the final price for order number 24144346?\"\n",
    "                    - Response: \"The final price for order number 24144346 is $500.\"\n",
    "\n",
    "                    2. Query: \"Hi, how are you?\"\n",
    "                    - Response: \"Hello! I’m doing great, thank you. How can I assist you today?\"\n",
    "\n",
    "                    3. Query: \"What is the capital of France?\"\n",
    "                    - Response: \"The capital of France is Paris.\"\n",
    "\n",
    "                    4. Query: \"Can you explain the total due amount in invoice number 56789?\"\n",
    "                    - Response: \"The total due amount in invoice number 56789 is $1,200.\"\n",
    "\n",
    "                    For queries unrelated to the document, answer in a general manner without referencing any document content.\n",
    "                    \"\"\"\n",
    "        },\n",
    "    ]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "response = llm_response(query, top_chunks)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import json\n",
    "\n",
    "# url = \"https://eeg-backend-hfehdmd4hxfagsgu.canadacentral-01.azurewebsites.net/api/users/product\"\n",
    "\n",
    "# response = requests.get(url)\n",
    "\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open ('data.json', 'w') as f:\n",
    "# \tjson.dump(response.json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over every item in the json and write it in a separate text file. each item will be a separate text file in a folder\n",
    "# for item in response.json():\n",
    "# \twith open(f\"{item['product_id']}.txt\", \"w\") as f:\n",
    "# \t\tfor i in item:\n",
    "# \t\t\tf.write(f\"{i}: {item[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
